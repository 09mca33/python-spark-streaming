{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretized Streams (DStreams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Discretized Streams\n",
    "\n",
    "**Discretized Streams** (or **DStreams**) are the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark’s abstraction of an immutable, distributed dataset (see Spark Programming Guide for more details). Each RDD in a DStream contains data from a certain interval, as shown in the following figure.\n",
    "\n",
    "![Spark Streaming Data Flow](https://spark.apache.org/docs/latest/img/streaming-dstream.png)\n",
    "\n",
    "Any operation applied on a DStream translates to operations on the underlying RDDs. For example, in the earlier example of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. This is shown in the following figure.\n",
    "\n",
    "![Spark Streaming Data Flow](https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png)\n",
    "\n",
    "\n",
    "These underlying RDD transformations are computed by the Spark engine. The DStream operations hide most of these details and provide the developer with a higher-level API for convenience. These operations are discussed in detail in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create Discretized Streams\n",
    "Input DStreams are DStreams representing the stream of input data received from streaming sources. In the quick example, lines was an input DStream as it represented the stream of data received from the netcat server. Every input DStream (except file stream, discussed later in this section) is associated with a Receiver (Scala doc, Java doc) object which receives the data from a source and stores it in Spark’s memory for processing.\n",
    "\n",
    "Spark Streaming provides two categories of built-in streaming sources.\n",
    "\n",
    "* Basic sources: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.\n",
    "* Advanced sources: Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the linking section.\n",
    "We are going to discuss some of the sources present in each category later in this section.\n",
    "\n",
    "Note that, if you want to receive multiple streams of data in parallel in your streaming application, you can create multiple input DStreams (discussed further in the Performance Tuning section). This will create multiple receivers which will simultaneously receive multiple data streams. But note that a Spark worker/executor is a long-running task, hence it occupies one of the cores allocated to the Spark Streaming application. Therefore, it is important to remember that a Spark Streaming application needs to be allocated enough cores (or threads, if running locally) to process the received data, as well as to run the receiver(s).\n",
    "\n",
    "**Points to remember**\n",
    "\n",
    "* When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL. Either of these means that only one thread will be used for running tasks locally. If you are using an input DStream based on a receiver (e.g. sockets, Kafka, Flume, etc.), then the single thread will be used to run the receiver, leaving no thread for processing the received data. Hence, when running locally, always use “local[n]” as the master URL, where n > number of receivers to run (see Spark Properties for information on how to set the master).\n",
    "\n",
    "* Extending the logic to running on a cluster, the number of cores allocated to the Spark Streaming application must be more than the number of receivers. Otherwise the system will receive data, but not be able to process it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different ways to create DStreams\n",
    "We have already taken a look at the `ssc.socketTextStream(...)` in the quick example which creates a DStream from text data received over a TCP socket connection. Besides sockets, the StreamingContext API provides methods for creating DStreams from files as input sources.\n",
    "\n",
    "* **File Streams:** For reading data from files on any file system compatible with the HDFS API (that is, HDFS, S3, NFS, etc.), a DStream can be created as:\n",
    "```python\n",
    "streamingContext.textFileStream(dataDirectory)\n",
    "```\n",
    "Spark Streaming will monitor the directory dataDirectory and process any files created in that directory (files written in nested directories not supported). Note that\n",
    "\n",
    "    * The files must have the same data format.\n",
    "    * files must be created in the dataDirectory by atomically moving or renaming them into the data directory.\n",
    "    * Once moved, the files must not be changed. So if the files are being continuously appended, the new data will not be read.\n",
    "For simple text files, there is an easier method `streamingContext.textFileStream(dataDirectory)`. And file streams do not require running a receiver, hence does not require allocating cores.\n",
    "* **Streams based on Custom Receivers:**DStreams can be created with data streams received through custom receivers. See the Custom Receiver Guide for more details.\n",
    "\n",
    "* **Queue of RDDs as a Stream:** For testing a Spark Streaming application with test data, one can also create a DStream based on a queue of RDDs, using `streamingContext.queueStream(queueOfRDDs)`. Each RDD pushed into the queue will be treated as a batch of data in the DStream, and processed like a stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full list of DStream Sources\n",
    "* File Streams\n",
    "* Queue of RDDs as a Stream\n",
    "* Kafka\n",
    "* Flume\n",
    "* Kinesis\n",
    "* Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting our first Spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Twitter section\n",
    "import socket\n",
    "import sys\n",
    "import requests\n",
    "import requests_oauthlib\n",
    "import json\n",
    "\n",
    "ACCESS_TOKEN = '2988835149-OgWXTLsBiosW74ZJi563l64WQ8f6tNudQKpLudp'\n",
    "ACCESS_SECRET = 'eOpeUX1wqL5sq9UK7yFbOPZ7ydYRZVqJ2Q2W7w3b1si7V'\n",
    "CONSUMER_KEY = 'AYYI7CvstplEi3fBAJ24vLVBA'\n",
    "CONSUMER_SECRET = 'OGFsmodX5DnHBcmZA3OrwFIeXS0gSUfPXcZVGTUOdffItb5Z0N'\n",
    "\n",
    "my_auth = requests_oauthlib.OAuth1(CONSUMER_KEY, CONSUMER_SECRET,ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "\n",
    "def send_tweets_to_spark(http_resp, tcp_connection):\n",
    "    for line in http_resp.iter_lines():\n",
    "        try:\n",
    "            full_tweet = json.loads(line)\n",
    "            tweet_text = full_tweet['text']\n",
    "            print(\"Tweet Text: \" + tweet_text)\n",
    "            print (\"------------------------------------------\")\n",
    "            tcp_connection.send(tweet_text + '\\n')\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(\"Error: %s\" % e)\n",
    "\n",
    "\n",
    "def get_tweets():\n",
    "    url = 'https://stream.twitter.com/1.1/statuses/filter.json'\n",
    "    query_data = [('locations', '-130,-20,100,50'), ('track', '#')]\n",
    "    query_url = url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in query_data])\n",
    "    response = requests.get(query_url, auth=my_auth, stream=True)\n",
    "    print(query_url, response)\n",
    "    return response\n",
    "\n",
    "\n",
    "TCP_IP = \"localhost\"\n",
    "TCP_PORT = 9009\n",
    "conn = None\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.bind((TCP_IP, TCP_PORT))\n",
    "s.listen(1)\n",
    "print(\"Waiting for TCP connection...\")\n",
    "conn, addr = s.accept()\n",
    "print(\"Connected... Starting getting tweets.\")\n",
    "resp = get_tweets()\n",
    "send_tweets_to_spark(resp,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Spark Section\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext # From within pyspark or send to spark-submit\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterStreamApp\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, 300) # 5 minute batch interval\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "dataStream = ssc.socketTextStream(\"localhost\",9009) # Stream IP (localhost), and port (5555 in our case)\n",
    "dataStream.pprint() # Print the incoming tweets to the console\n",
    "\n",
    "ssc.start() # Start reading the stream\n",
    "ssc.awaitTermination() # Wait for the process to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "For testing a Spark Streaming application with test data, one can also create a DStream based on a queue of RDDs, using `streamingContext.queueStream(queueOfRDDs)`. Each RDD pushed into the queue will be treated as a batch of data in the DStream, and processed like a stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\"\"\"\n",
    " Create a queue of RDDs that will be mapped/reduced one at a time in\n",
    " 1 second intervals.\n",
    " To run this example use\n",
    "    `$ bin/spark-submit examples/src/main/python/streaming/queue_stream.py\n",
    "\"\"\"\n",
    "import time\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n",
    "    ssc = StreamingContext(sc, 1)\n",
    "    # Create the queue through which RDDs can be pushed to\n",
    "    # a QueueInputDStream\n",
    "    rddQueue = []\n",
    "    for i in range(5):\n",
    "        rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]\n",
    "    # Create the QueueInputDStream and use it do some processing\n",
    "    inputStream = ssc.queueStream(rddQueue)\n",
    "    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n",
    "    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n",
    "    reducedStream.pprint()\n",
    "    ssc.start()\n",
    "    time.sleep(6)\n",
    "    ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#basic-sources\n",
    "2. https://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams\n",
    "3. https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext\n",
    "4. https://github.com/vaquarkhan/vk-wiki-notes/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
