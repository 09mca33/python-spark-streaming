{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Transformations Demo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed earlier, there are a wide variety of data transformations available for use on DStreams, most of which are similar to those used on the DStreams' constituent parts.\n",
    "\n",
    "As a reminder, here is the list of transformations from the previous demo again:\n",
    "\n",
    "| Transformation        | Meaning         |\n",
    "| ------------------------------ |:-------------|\n",
    "| **map**(func)      | Return a new DStream by passing each element of the source DStream through a function func.    |\n",
    "| **flatMap**(func)\t| Similar to map, but each input item can be mapped to 0 or more output items.    |\n",
    "| **filter**(func)\t| Return a new DStream by selecting only the records of the source DStream on which func returns true.    |\n",
    "| **repartition**(numPartitions)\t| Changes the level of parallelism in this DStream by creating more or fewer partitions.    |\n",
    "| **union**(otherStream)\t| Return a new DStream that contains the union of the elements in the source DStream and otherDStream. |\n",
    "| **count**()\t| Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.  |\n",
    "| **reduce**(func)\t| Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using  a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.\n",
    "| **countByValue**()\t| When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.\n",
    "| **reduceByKey**(func, [numTasks])\t| When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n",
    "| **join**(otherStream, [numTasks])\t| When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "| **cogroup**(otherStream, [numTasks])\t| When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.\n",
    "\n",
    "\n",
    "If you look at the spark streaming documentation, you will also find the `transform(func)` and `updateStateByKey(func)`. We will discuss these later in the course.\n",
    "\n",
    "\n",
    "Let's go though another example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Last time we went over the `map` and `flapmap` functions. We'll explore a few other options.\n",
    "\n",
    "Suppose we have a this example text from Dr Suess's _The Cat in the Hat_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8b5aca44da72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Its fun to have fun,'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'but you have to know how.'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Suppose then that we want to get wordcounts for this. We can use the map function from before here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# map returns a new RDD containing values created by applying the supplied function to each value in the original RDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Here we use a lambda function which replaces some common punctuation characters with spaces and convert to lower\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# case, producing a new RDD:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "scc = Streamingcontext(\"local[2]\",\"PythonSparkApp\", 10)\n",
    "\n",
    "myFile = scc.sparkContext.textFile(\"..data/DrSeuss.txt\")\n",
    "wordspair = myFile.flatMap(lambda row: row.split(\" \")).map(lambda x: (x, 1)).reduceByKey(lambda x,y : x + y)\n",
    "oldwordcount = wordspair.reduceByKey(lambda x,y : x + y)\n",
    "lines = scc.socketTextStream(\"192.168.56.101\", 9999)\n",
    "\n",
    "lines = sc.parallelize(['Its fun to have fun,','but you have to know how.'])\n",
    "# Suppose then that we want to get wordcounts for this. We can use the map function from before here. \n",
    "# map returns a new RDD containing values created by applying the supplied function to each value in the original RDD\n",
    "# Here we use a lambda function which replaces some common punctuation characters with spaces and convert to lower \n",
    "# case, producing a new RDD:\n",
    "\n",
    "wordcounts1 = lines.map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\n",
    "wordcounts1.take(10)\n",
    "pprint(wordcounts1)\n",
    "\n",
    "# The flatMap function takes these input values and returns a new, flattened list. In this case, the lines are split \n",
    "# into words and then each word becomes a separate value in the output RDD:\n",
    "\n",
    "wordcounts2 = wordcounts1.flatMap(lambda x: x.split())\n",
    "wordcounts2.take(20)\n",
    "pprint(wordcounts2)\n",
    "\n",
    "# Expect that the input RDD contains tuples of the form (key,value). Create a new RDD containing a tuple for \n",
    "# each unique value of key in the input, where the value in the second position of the tuple is created by \n",
    "# applying the supplied lambda function to the values with the matching key in the input RDD\n",
    "# Here the key will be the word and lambda function will sum up the word counts for each word. The output RDD \n",
    "# will consist of a single tuple for each unique word in the data, where the word is stored at the first position \n",
    "# in the tuple and the word count is stored at the second position\n",
    "\n",
    "wordcounts3 = wordcounts2.map(lambda x: (x, 1))\n",
    "wordcounts3.take(20)\n",
    "pprint(wordcounts3)\n",
    "\n",
    "wordcounts4 = wordcounts3.reduceByKey(lambda x,y:x+y)\n",
    "wordcounts4.take(20)\n",
    "pprint(wordcounts4)\n",
    "\n",
    "# map a lambda function to the data which will swap over the first and second values in each tuple, now the word count\n",
    "# appears in the first position and the word in the second position\n",
    "\n",
    "wordcounts5 = wordcounts4.map(lambda x:(x[1],x[0]))\n",
    "wordcounts5.take(20)\n",
    "pprint(wordcounts5)\n",
    "\n",
    "# we sort the input RDD by the key value (i.e., the value at the first position in each tuple)\n",
    "# In this example the first position stores the word count so this will sort the words so that the \n",
    "# most frequently occurring words occur first in the RDD. The ascending=False parameter results in a descending sort order\n",
    "\n",
    "wordcounts6 = wordcounts5.sortByKey(ascending=False)\n",
    "wordcounts6.take(20)\n",
    "pprint(wordcounts6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
