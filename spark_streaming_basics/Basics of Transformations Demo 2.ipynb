{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Transformations Demo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed earlier, there are a wide variety of data transformations available for use on DStreams, most of which are similar to those used on the DStreams' constituent parts.\n",
    "\n",
    "As a reminder, here is the list of transformations from the previous demo again:\n",
    "\n",
    "| Transformation        | Meaning         |\n",
    "| ------------------------------ |:-------------|\n",
    "| **map**(func)      | Return a new DStream by passing each element of the source DStream through a function func.    |\n",
    "| **flatMap**(func)\t| Similar to map, but each input item can be mapped to 0 or more output items.    |\n",
    "| **filter**(func)\t| Return a new DStream by selecting only the records of the source DStream on which func returns true.    |\n",
    "| **repartition**(numPartitions)\t| Changes the level of parallelism in this DStream by creating more or fewer partitions.    |\n",
    "| **union**(otherStream)\t| Return a new DStream that contains the union of the elements in the source DStream and otherDStream. |\n",
    "| **count**()\t| Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.  |\n",
    "| **reduce**(func)\t| Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using  a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.\n",
    "| **countByValue**()\t| When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.\n",
    "| **reduceByKey**(func, [numTasks])\t| When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n",
    "| **join**(otherStream, [numTasks])\t| When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "| **cogroup**(otherStream, [numTasks])\t| When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.\n",
    "| **transform**(func)\t| Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream.\n",
    "| **updateStateByKey**(func)\t| Return a new \"state\" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.\n",
    "\n",
    "\n",
    "Let's go though another example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Last time we went over the `map` and `flapmap` functions. We'll explore a few other options.\n",
    "\n",
    "Suppose we have a this example text from Dr Suess's _The Cat in the Hat_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.parallelize(['Its fun to have fun,','but you have to know how.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose then that we want to get wordcounts for this. We can use the map function from before here. map returns a new RDD containing values created by applying the supplied function to each value in the original RDD\n",
    "\n",
    "Here we use a lambda function which replaces some common punctuation characters with spaces and convert to lower case, producing a new RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts1 = lines.map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\n",
    "wordcounts1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flatMap  function takes these input values and returns a new, flattened list. In this case, the lines are split into words and then each word becomes a separate value in the output RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts2 = wordcounts1.flatMap(lambda x: x.split())\n",
    "wordcounts2.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second map invocation, we use a function which replaces each original value in the input RDD with a 2-tuple containing the word in the first position and the integer value 1 in the second position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts3 = wordcounts2.map(lambda x: (x, 1))\n",
    "wordcounts3.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect that the input RDD contains tuples of the form `(key,value)`. Create a new RDD containing a tuple for each unique value of `key` in the input, where the value in the second position of the tuple is created by applying the supplied lambda function to the `value`s with the matching `key` in the input RDD\n",
    "\n",
    "Here the key will be the word and lambda function will sum up the word counts for each word. The output RDD will consist of a single tuple for each unique word in the data, where the word is stored at the first position in the tuple and the word count is stored at the second position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts4 = wordcounts3.reduceByKey(lambda x,y:x+y)\n",
    "wordcounts4.take(20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map a lambda function to the data which will swap over the first and second values in each tuple, now the word count appears in the first position and the word in the second position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts5 = wordcounts4.map(lambda x:(x[1],x[0]))\n",
    "wordcounts5.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sort the input RDD by the key value (i.e., the value at the first position in each tuple)\n",
    "\n",
    "In this example the first position stores the word count so this will sort the words so that the most frequently occurring words occur first in the RDD. The `ascending=False` parameter results in a descending sort order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts6 = wordcounts5.sortByKey(ascending=False)\n",
    "wordcounts6.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have our sorted wordcount for this incredibly small poem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
