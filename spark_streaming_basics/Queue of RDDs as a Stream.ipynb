{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Discretized Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discretized Streams** (or **DStreams**) are the basic abstraction provided by Spark Streaming. These are continuous streams of data. The DStream could be the input coming from a source, or the output data that was generated by performing functions on the input. DStreams are basically continuous series of RDDs, which is Spark’s abstraction of an immutable, distributed dataset. \n",
    "\n",
    "One of the consequences of this is that any operation applied on a DStream translates to operations on the underlying RDDs. For example, in the earlier example of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream.\n",
    "\n",
    "Spark Streaming provides two categories of built-in streaming sources.\n",
    "\n",
    "* Basic sources: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.\n",
    "* Advanced sources: Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. We will go into more depth in these in later sections of the course.\n",
    "\n",
    "If you want to receive multiple streams of data in parallel, you can create multiple input DStreams. This will create multiple receivers which will simultaneously receive multiple data streams. However, it is important to remember that a Spark Streaming application needs to be allocated enough cores to process the received data, as well as to run the receiver.\n",
    "\n",
    "Besides TCP sockets, the StreamingContext API provides methods for creating DStreams from files as input sources.\n",
    "\n",
    "* **File Streams:** For reading data from files on any file system compatible with the HDFS API (that is, HDFS, S3, NFS, etc.), a DStream can be created as `streamingContext.textFileStream(dataDirectory)`. Spark Streaming will monitor the directory dataDirectory and process any files created in that directory (files written in nested directories not supported). It's worth noting that 1) The files must have the same data format, 2) files must be created in the dataDirectory by atomically moving or renaming them into the data directory, and 3) once moved, the files must not be changed. So if the files are being continuously appended, the new data will not be read. For simple text files, there is also an easier method `streamingContext.textFileStream(dataDirectory)`. Since file streams do not require running a receiver, they do not require allocating cores for cluster computing.\n",
    "\n",
    "* **Queue of RDDs as a Stream:** For testing a Spark Streaming application with test data, one can also create a DStream based on a queue of RDDs, using `streamingContext.queueStream(queueOfRDDs)`. Each RDD pushed into the queue will be treated as a batch of data in the DStream, and processed like a stream\n",
    "\n",
    "* **Streams based on Custom Receivers:** DStreams can be created with data streams received through custom receivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "For testing a Spark Streaming application with test data, we are going to create a DStream based on a queue of RDDs, using `streamingContext.queueStream(queueOfRDDs)`. Each RDD pushed into the queue will be treated as a batch of data in the DStream, and processed like a stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n",
    "    ssc = StreamingContext(sc, 1)\n",
    "    \n",
    "    \n",
    "    rddQueue = []\n",
    "    for i in range(5):\n",
    "        rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]\n",
    "    \n",
    "    inputStream = ssc.queueStream(rddQueue)\n",
    "    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n",
    "    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n",
    "    reducedStream.pprint()\n",
    "    \n",
    "    ssc.start()\n",
    "    time.sleep(6)\n",
    "    ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**important notes**\n",
    "\n",
    "* When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL. Either of these means that only one thread will be used for running tasks locally. If you are using an input DStream based on a receiver (e.g. sockets, Kafka, Flume, etc.), then the single thread will be used to run the receiver, leaving no thread for processing the received data. Hence, when running locally, always use “local[n]” as the master URL, where n > number of receivers to run (see Spark Properties for information on how to set the master).\n",
    "\n",
    "* Extending the logic to running on a cluster, the number of cores allocated to the Spark Streaming application must be more than the number of receivers. Otherwise the system will receive data, but not be able to process it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#basic-sources\n",
    "2. https://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams\n",
    "3. https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
