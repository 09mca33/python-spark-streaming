{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulators Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "# Get or register a Broadcast variable\n",
    "def getWordBlacklist(sparkContext):\n",
    "    if ('wordBlacklist' not in globals()):\n",
    "        globals()['wordBlacklist'] = sparkContext.broadcast([\"a\", \"b\", \"c\"])\n",
    "    return globals()['wordBlacklist']\n",
    "\n",
    "\n",
    "# Get or register an Accumulator\n",
    "def getDroppedWordsCounter(sparkContext):\n",
    "    if ('droppedWordsCounter' not in globals()):\n",
    "        globals()['droppedWordsCounter'] = sparkContext.accumulator(0)\n",
    "    return globals()['droppedWordsCounter']\n",
    "\n",
    "\n",
    "def createContext(host, port, outputPath):\n",
    "    # If you do not see this printed, that means the StreamingContext has been loaded\n",
    "    # from the new checkpoint\n",
    "    print(\"Creating new context\")\n",
    "    if os.path.exists(outputPath):\n",
    "        os.remove(outputPath)\n",
    "    sc = SparkContext(appName=\"PythonStreamingRecoverableNetworkWordCount\")\n",
    "    ssc = StreamingContext(sc, 1)\n",
    "\n",
    "    # Create a socket stream on target ip:port and count the\n",
    "    # words in input stream of \\n delimited text (eg. generated by 'nc')\n",
    "    lines = ssc.socketTextStream(host, port)\n",
    "    words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "    wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "    def echo(time, rdd):\n",
    "        # Get or register the blacklist Broadcast\n",
    "        blacklist = getWordBlacklist(rdd.context)\n",
    "        # Get or register the droppedWordsCounter Accumulator\n",
    "        droppedWordsCounter = getDroppedWordsCounter(rdd.context)\n",
    "\n",
    "        # Use blacklist to drop words and use droppedWordsCounter to count them\n",
    "        def filterFunc(wordCount):\n",
    "            if wordCount[0] in blacklist.value:\n",
    "                droppedWordsCounter.add(wordCount[1])\n",
    "                False\n",
    "            else:\n",
    "                True\n",
    "\n",
    "        counts = \"Counts at time %s %s\" % (time, rdd.filter(filterFunc).collect())\n",
    "        print(counts)\n",
    "        print(\"Dropped %d word(s) totally\" % droppedWordsCounter.value)\n",
    "        print(\"Appending to \" + os.path.abspath(outputPath))\n",
    "        with open(outputPath, 'a') as f:\n",
    "            f.write(counts + \"\\n\")\n",
    "\n",
    "    wordCounts.foreachRDD(echo)\n",
    "    return ssc\n",
    "\n",
    "\n",
    "host, port, checkpoint, output = sys.argv[1:]\n",
    "ssc = StreamingContext.getOrCreate(checkpoint,lambda: createContext(host, int(port), output))\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#accumulators-broadcast-variables-and-checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
