{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Operations on DStreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output operations allow DStreamâ€™s data to be pushed out to external systems like a database or a file systems. Since the output operations actually allow the transformed data to be consumed by external systems, they trigger the actual execution of all the DStream transformations (similar to actions for RDDs). Currently, the following output operations are defined:\n",
    "\n",
    "| Output Operation        | Meaning           |\n",
    "|-------------:|:------------- |\n",
    "| **pprint**()      | Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. |\n",
    "| **saveAsTextFiles**(prefix, [suffix])     | Save this DStream's contents as text files. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\". |\n",
    "| **foreachRDD**(func) | The most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Java and Scala APIs for Spark, there are also the `saveAsObjectFiles` and `saveAsHadoopFiles`. Unfortunately, these are not available in the Python API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "The following is a demonstration of how to output RDDs. Once could easily use `statuses.saveAsTextFiles(\"Tweets\", \"txt\")` to achieve this, but we will limit the number of tweets to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# TODO: your path will likely not have 'matthew' in it. Change it to reflect your path.\n",
    "findspark.init('/home/matthew/spark-2.1.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.streaming\n",
    "from pyspark.streaming import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import utils\n",
    "\n",
    "\n",
    "# Configure Twitter credentials using twitter.txt\n",
    "setupTwitter()\n",
    "    \n",
    "# Set up a Spark streaming context named \"SaveTweets\" that runs locally using\n",
    "# all CPU cores and one-second batches of data\n",
    "ssc = StreamingContext(\"local[*]\", \"SaveTweets\", Seconds(1))\n",
    "    \n",
    "# Get rid of log spam (should be called after the context is set up)\n",
    "setupLogging()\n",
    "\n",
    "# Create a DStream from Twitter using our streaming context\n",
    "tweets = TwitterUtils.createStream(ssc, None)\n",
    "    \n",
    "# Now extract the text of each status update into RDD's using map()\n",
    "statuses = tweets.map(lambda status: status.getText())\n",
    "\n",
    "totalTweets = int(0)\n",
    "        \n",
    "def twitterStatus(rdd, time):\n",
    "    \n",
    "    # Don't bother with empty batches\n",
    "    if rdd.count() > 0:\n",
    "    \n",
    "        # Combine each partition's results into a single RDD:\n",
    "        repartitionedRDD = rdd.repartition(1).cache()\n",
    "        # And print out a directory with the results.\n",
    "        repartitionedRDD.saveAsTextFile(\"Tweets_\" + time.milliseconds.toString)\n",
    "        \n",
    "        # Stop once we've collected 1000 tweets.\n",
    "        totalTweets += repartitionedRDD.count()\n",
    "        print(\"Tweet count: \" + totalTweets)\n",
    "        if totalTweets > 1000:\n",
    "            sys.exit(0)\n",
    "    \n",
    "statuses.foreachRDD(twitterStatus(rdd, time))\n",
    "    \n",
    "ssc.checkpoint(\"..checkpoint/\")\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#output-operations-on-dstreams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
